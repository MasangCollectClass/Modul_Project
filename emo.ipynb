{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fdac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
      "     -------------------------------------- 755.0/755.0 KB 3.7 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "     -------------------------------------- 491.5/491.5 KB 4.4 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "     -------------------------------------- 100.9/100.9 KB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from openai) (4.14.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 73.5/73.5 KB ? eta 0:00:00\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp39-cp39-win_amd64.whl (208 kB)\n",
      "     -------------------------------------- 208.8/208.8 KB 4.2 MB/s eta 0:00:00\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "     -------------------------------------- 444.8/444.8 KB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "     -------------------------------------- 515.4/515.4 KB 6.5 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 KB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (25.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.4/133.4 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "     -------------------------------------- 162.3/162.3 KB 4.9 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ------------------------------------- 193.6/193.6 KB 12.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.13-cp39-cp39-win_amd64.whl (451 kB)\n",
      "     -------------------------------------- 451.7/451.7 KB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.8/78.8 KB 4.6 MB/s eta 0:00:00\n",
      "Collecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.1/44.1 KB 2.1 MB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp39-cp39-win_amd64.whl (42 kB)\n",
      "     ---------------------------------------- 42.0/42.0 KB 2.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp39-cp39-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.1/46.1 KB ? eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp39-cp39-win_amd64.whl (87 kB)\n",
      "     ---------------------------------------- 87.4/87.4 KB 2.5 MB/s eta 0:00:00\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, typing-inspection, tqdm, sniffio, pyyaml, python-dotenv, pydantic-core, propcache, multidict, jiter, h11, fsspec, frozenlist, filelock, distro, dill, async-timeout, annotated-types, aiohappyeyeballs, yarl, pydantic, multiprocess, huggingface-hub, httpcore, anyio, aiosignal, httpx, aiohttp, openai, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 distro-1.9.0 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.33.2 jiter-0.10.0 multidict-6.6.3 multiprocess-0.70.16 openai-1.93.0 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 pyyaml-6.0.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script distro.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts huggingface-cli.exe and tiny-agents.exe are installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openai python-dotenv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c3422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 클라이언트가 성공적으로 초기화되었습니다.\n",
      "--- MBTI 상담 챗봇 단일 감정 분석 시작 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[챗봇 분석 중...]\n",
      "    > 분석된 감정: 중립\n",
      "--------------------------------------------------\n",
      "감정 분석 프로그램을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset # datasets 라이브러리 임포트\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "client = None\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(\"OpenAI 클라이언트가 성공적으로 초기화되었습니다.\")\n",
    "    # (선택 사항) API 연결 확인을 위한 간단한 테스트:\n",
    "    # models_response = client.models.list()\n",
    "    # print(f\"API 연결 확인. 사용 가능한 모델 예시: {[m.id for m in models_response.data[:3]]}\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: OpenAI 클라이언트 초기화 실패: {e}\") # 실제 오류 메시지 출력\n",
    "    print(f\"'.env' 파일에 'OPENAI_API_KEY'가 설정되었는지, 또는 키가 유효한지 확인해주세요.\")\n",
    "    print(\"이 오류가 발생하면 이후 감정 분석 기능이 정상 작동하지 않습니다.\")\n",
    "\n",
    "def analyze_sentiment(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    사용자의 텍스트(한국어 또는 영어)를 입력받아 GPT API를 통해 감정을 분석하고\n",
    "    미리 정의된 한국어 감정 목록 중 하나로 반환하는 함수.\n",
    "    이 함수는 go_emotions 데이터셋의 few-shot 예시를 활용합니다.\n",
    "\n",
    "    Args:\n",
    "        user_text (str): 분석할 사용자의 대화 텍스트 (한국어 또는 영어).\n",
    "\n",
    "    Returns:\n",
    "        str: 분석된 감정 (예: '기쁨', '슬픔', '중립' 등).\n",
    "             GPT가 정해진 감정 외 다른 것을 반환하거나, API 호출에 실패할 경우 '중립' 또는 '오류' 반환.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        print(\"경고: OpenAI 클라이언트가 초기화되지 않아 감정 분석을 수행할 수 없습니다. API 키 설정을 확인해주세요.\")\n",
    "        return \"오류\"\n",
    "\n",
    "    allowed_emotions = [\n",
    "        # 🌞 긍정\n",
    "        \"기쁨\",\"환희\",\"유쾌\",\"희열\",\"만족\",\"뿌듯함\",\"감사\",\"사랑\",\"애정\",\n",
    "        \"평온\",\"안도\",\"자신감\",\"설렘\",\"기대\",\"존경\",\"경외\",\"흥분\",\"열정\",\"유머\", # 열정, 유머 추가됨\n",
    "        # 🌧 부정\n",
    "        \"슬픔\",\"우울\",\"상실감\",\"실망\",\"좌절\",\"후회\",\"자책\",\"외로움\",\"분노\",\"짜증\",\n",
    "        \"혐오\",\"경멸\",\"두려움\",\"공포\",\"불안\",\"초조\",\"수치\",\"부끄러움\",\"피로\",\"무기력\",\"억울함\", # 무기력, 억울함 추가됨\n",
    "        # 🌪 혼합·복합\n",
    "        \"혼란\",\"갈등\",\"당황\",\"놀람\",\"충격\",\"호기심\",\"의심\",\"불신\",\"그리움\",\"향수\", # 충격, 불신, 향수 추가됨\n",
    "        # 💤 기타\n",
    "        \"중립\"\n",
    "    ]\n",
    "\n",
    "    # --- GoEmotions 데이터셋 로드 및 전처리 ---\n",
    "    # 데이터셋의 English 감정 레이블을 한국어 레이블로 매핑\n",
    "    # 이 매핑은 GoEmotions 데이터셋의 28개 감정 레이블을 기반으로 합니다.\n",
    "    # 각 영문 감정에 가장 적합한 한국어 감정을 매칭시켜야 합니다.\n",
    "    # allowed_emotions 리스트에 없는 감정은 '중립' 등으로 폴백 처리하거나,\n",
    "    # allowed_emotions 리스트를 확장해야 합니다.\n",
    "    go_emotions_to_korean_map = {\n",
    "        'admiration': '존경',\n",
    "        'amusement': '유머',\n",
    "        'anger': '분노',\n",
    "        'annoyance': '짜증',\n",
    "        'approval': '만족',\n",
    "        'caring': '사랑',\n",
    "        'confusion': '혼란',\n",
    "        'curiosity': '호기심',\n",
    "        'desire': '기대',\n",
    "        'disappointment': '실망',\n",
    "        'disapproval': '불신',\n",
    "        'disgust': '혐오',\n",
    "        'embarrassment': '부끄러움',\n",
    "        'excitement': '흥분',\n",
    "        'fear': '두려움',\n",
    "        'gratitude': '감사',\n",
    "        'grief': '슬픔', # 강한 슬픔\n",
    "        'joy': '기쁨',\n",
    "        'love': '사랑',\n",
    "        'nervousness': '불안',\n",
    "        'optimism': '자신감', # 또는 '기대'\n",
    "        'pride': '뿌듯함',\n",
    "        'realization': '놀람', # 또는 '깨달음', 문맥에 따라\n",
    "        'relief': '안도',\n",
    "        'remorse': '후회',\n",
    "        'sadness': '슬픔',\n",
    "        'surprise': '놀람',\n",
    "        'neutral': '중립',\n",
    "        'disappointment': '실망', # 중복 방지. 다른 감정과의 관계 확인 필요.\n",
    "        'loneliness': '외로움',\n",
    "        'shame': '수치',\n",
    "        'fatigue': '피로',\n",
    "        'emptiness': '무기력',\n",
    "        'bewilderment': '당황',\n",
    "        'agitation': '초조',\n",
    "        'distress': '좌절',\n",
    "        'longing': '그리움',\n",
    "        'hope': '기대', # '설렘'과 비슷\n",
    "        'triumph': '희열',\n",
    "        'lively': '유쾌',\n",
    "        'contentment': '평온'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 데이터셋 로드\n",
    "        ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "        # GoEmotions 데이터셋의 레이블 이름 목록 (영어)\n",
    "        go_emotions_label_names = ds['train'].features['labels'].feature.names\n",
    "\n",
    "        # 프롬프트 구성 시작\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"너는 사용자의 감정을 분석하는 전문 심리 분석가야. 요청하는 문장에서 가장 지배적인 감정을 정확하게 식별하고, 오직 다음 한국어 감정 단어 목록 중 하나로만 반환해야 해: {', '.join(allowed_emotions)}. 만약 사용자 문장이 영어로 되어있다면, 분석된 영어 감정에 가장 가까운 한국어 감정 단어를 목록에서 찾아 반환해야 해. 다른 부가적인 설명, 문장, 인사는 일절 허용되지 않아.\"},\n",
    "            {\"role\": \"user\", \"content\": \"나는 너에게 한국어 또는 영어 문장을 제공할 거야. 너는 그 문장에서 가장 강하게 드러나는 감정을 위에 명시된 한국어 감정 단어 중 하나로, 오직 그 단어 자체만 출력해줘. 다른 불필요한 설명, 문장, 인사는 일절 포함하지 마.\"},\n",
    "        ]\n",
    "\n",
    "        # Few-shot 예시 데이터셋에서 가져오기\n",
    "        # 너무 많은 예시를 넣으면 토큰 비용이 늘어나고, GPT가 혼란스러워할 수 있으므로 적절한 개수만 사용\n",
    "        num_few_shot_examples = 100 # 사용할 few-shot 예시의 개수\n",
    "        added_examples_count = 0\n",
    "        for example in ds['train']:\n",
    "            # go_emotions 'simplified'는 보통 단일 레이블을 가집니다.\n",
    "            # 만약 다중 레이블이 있다면, 첫 번째 레이블만 사용하거나,\n",
    "            # (예: if len(example['labels']) == 1: ) 단일 레이블 예시만 필터링할 수 있습니다.\n",
    "            if example['labels'] and added_examples_count < num_few_shot_examples:\n",
    "                original_english_label_id = example['labels'][0]\n",
    "                original_english_label_name = go_emotions_label_names[original_english_label_id]\n",
    "                \n",
    "                # 영어 레이블을 한국어 레이블로 매핑\n",
    "                korean_emotion = go_emotions_to_korean_map.get(original_english_label_name, \"중립\")\n",
    "                \n",
    "                # 매핑된 한국어 감정 단어가 allowed_emotions 목록에 있는지 확인\n",
    "                if korean_emotion not in allowed_emotions:\n",
    "                    # 만약 매핑된 감정이 허용된 목록에 없다면 \"중립\"으로 폴백 (오류 방지)\n",
    "                    print(f\"경고: 매핑된 감정 '{korean_emotion}'(원문: {original_english_label_name})이 허용된 목록에 없습니다. '중립'으로 처리합니다.\")\n",
    "                    korean_emotion = \"중립\"\n",
    "\n",
    "                prompt_messages.append({\"role\": \"user\", \"content\": f\"문장: '{example['text']}'\"})\n",
    "                prompt_messages.append({\"role\": \"assistant\", \"content\": korean_emotion})\n",
    "                added_examples_count += 1\n",
    "            elif added_examples_count >= num_few_shot_examples:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"데이터셋 로드 또는 Few-shot 예시 생성 중 오류 발생: {e}\")\n",
    "        # 데이터셋 로드 실패 시에도 최소한의 프롬프트 유지 (하드코딩 예시 사용)\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"너는 사용자의 감정을 분석하는 전문 심리 분석가야. 요청하는 문장에서 가장 지배적인 감정을 정확하게 식별하고, 오직 다음 한국어 감정 단어 목록 중 하나로만 반환해야 해: {', '.join(allowed_emotions)}. 만약 사용자 문장이 영어로 되어있다면, 분석된 영어 감정에 가장 가까운 한국어 감정 단어를 목록에서 찾아 반환해야 해. 다른 부가적인 설명, 문장, 인사는 일절 허용되지 않아.\"},\n",
    "            {\"role\": \"user\", \"content\": \"나는 너에게 한국어 또는 영어 문장을 제공할 거야. 너는 그 문장에서 가장 강하게 드러나는 감정을 위에 명시된 한국어 감정 단어 중 하나로, 오직 그 단어 자체만 출력해줘. 다른 불필요한 설명, 문장, 인사는 일절 포함하지 마.\"},\n",
    "            # 데이터셋 로드 실패 시 fallback으로 사용할 하드코딩 예시 (원래 코드의 예시들을 여기에 추가)\n",
    "            {\"role\": \"user\", \"content\": \"문장: '오늘 정말 기분이 최고야!'\"}, {\"role\": \"assistant\", \"content\": \"기쁨\"},\n",
    "            {\"role\": \"user\", \"content\": \"문장: 'I feel incredibly happy today!'\"}, {\"role\": \"assistant\", \"content\": \"기쁨\"},\n",
    "            {\"role\": \"user\", \"content\": \"문장: 'Why am I so sad?'\"}, {\"role\": \"assistant\", \"content\": \"슬픔\"},\n",
    "            {\"role\": \"user\", \"content\": \"문장: 'This is completely neutral'\"}, {\"role\": \"assistant\", \"content\": \"중립\"},\n",
    "        ]\n",
    "        print(\"데이터셋 로드에 실패하여 기본 하드코딩 예시로 전환합니다.\")\n",
    "\n",
    "\n",
    "    # 실제 분석 대상 문장을 prompt_messages 리스트에 추가\n",
    "    prompt_messages.append({\"role\": \"user\", \"content\": f\"문장: '{user_text}'\"})\n",
    "\n",
    "    # 사용할 GPT 모델 선택:\n",
    "    # \"gpt-4o\" 또는 \"gpt-4\"가 언어 혼용 처리 및 정확한 감정 분석에 더 유리합니다.\n",
    "    model_to_use = \"gpt-4o\"\n",
    "\n",
    "    temperature = 0.0 # 0.0으로 하여 GPT가 지시를 가장 엄격하게 따르도록 함.\n",
    "    max_tokens = 5 # 최소 토큰으로만 단어 하나만 생성하도록 제한.\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_to_use,\n",
    "            messages=prompt_messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        sentiment = response.choices[0].message.content.strip()\n",
    "\n",
    "        if sentiment in allowed_emotions:\n",
    "            return sentiment\n",
    "        else:\n",
    "            print(f\"경고: GPT가 예상치 못한 감정 '{sentiment}'을(를) 반환했습니다. '중립'으로 처리합니다.\")\n",
    "            return \"중립\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"감정 분석 중 GPT API 호출 오류 발생: {e}\")\n",
    "        return \"중립\"\n",
    "\n",
    "print(\"--- MBTI 상담 챗봇 단일 감정 분석 시작 ---\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\n당신의 대화를 입력해주세요 (종료하려면 '종료' 입력): \")\n",
    "\n",
    "    if user_input.lower() == '종료':\n",
    "        print(\"감정 분석 프로그램을 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\n[챗봇 분석 중...]\")\n",
    "\n",
    "    detected_emotion = analyze_sentiment(user_input)\n",
    "    print(f\"    > 분석된 감정: {detected_emotion}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94904a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
